{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-IUxJ0pt8D93"
      },
      "source": [
        "# TFRS\n",
        "\n",
        "This notebook takes the master data that was prepared in the \"data_preprocessing\" notebook, creates a test holdout set from the 20% of the data that the same split are used for other experimentations for consistency.\n",
        "\n",
        "The model is created by TensorFlow Recommenders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VEWiCCw0Nkg_"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-recommenders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2xnqHu9X_JuV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "from typing import Dict, Text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wi_nO_5d8ryx"
      },
      "source": [
        "## Train-test Split\n",
        "\n",
        "The train test split is done by only taking the 20% of the data as the test holdout set. For making sure the train and test data is consistent in all experiments, the following test holdout split will be the same for each experiment.\n",
        "\n",
        "It is an important detail that the split is done in a stratified way to ensure that the user rankings will be splitted as evenly as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iOE86adV_TfG"
      },
      "outputs": [],
      "source": [
        "#Do not load the \"timestamp\" column since it is not needed for building the recommender engine\n",
        "df = pd.read_csv('data/master_data.zip', compression=\"zip\")[[\"userId\", \"movieId\", \"rating\"]]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O-27CJ9N8u-B"
      },
      "source": [
        "Data is transformed into TensorFlow dataset format "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-aOy9ffu_jM1"
      },
      "outputs": [],
      "source": [
        "trainset = tf.data.Dataset.from_tensor_slices(df.values)\n",
        "\n",
        "trainset = trainset.map(lambda x: {\n",
        "    \"user_id\": tf.as_string(tf.cast(x[0], tf.int32))  ,\n",
        "    \"movie_id\": tf.as_string(tf.cast(x[1], tf.int32)) ,\n",
        "    \"rating\": tf.cast(x[2], tf.float32)\n",
        "})\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kNTbi3vr85FA"
      },
      "source": [
        "Unique user and movie IDs are determined for embedding generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OYHm12yS_jPX"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['data/unique_user_ids.pkl']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movie_ids = trainset.batch(1_000_000).map(lambda x: x[\"movie_id\"])\n",
        "user_ids = trainset.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
        "\n",
        "import joblib\n",
        "unique_movie_ids = np.unique(np.concatenate(list(movie_ids)))\n",
        "joblib.dump(unique_movie_ids, \"data/unique_movie_ids.pkl\")\n",
        "#unique_movie_ids = joblib.load(\"data/unique_movie_ids.pkl\")\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
        "joblib.dump(unique_user_ids, \"data/unique_user_ids.pkl\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H31rIU1o-Lbw"
      },
      "source": [
        "## Model Build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rEFgWIEz_jR9"
      },
      "outputs": [],
      "source": [
        "class ModelRanking(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    embedding_dims = 32\n",
        "\n",
        "    # User embeddings\n",
        "    self.user_embeddings = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_user_ids, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dims)\n",
        "    ])\n",
        "\n",
        "    # Movie Embeddings\n",
        "    self.movie_embeddings = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_movie_ids, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_ids) + 1, embedding_dims)\n",
        "    ])\n",
        "\n",
        "    # Predictions\n",
        "    self.ratings = tf.keras.Sequential([\n",
        "      # multiple dense layers\n",
        "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "      # Ratings in output layer\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    user_id, movie_id = inputs\n",
        "\n",
        "    user_embed = self.user_embeddings(user_id)\n",
        "    movie_embed = self.movie_embeddings(movie_id)\n",
        "\n",
        "    return self.ratings(tf.concat([user_embed, movie_embed], axis=1))\n",
        "  \n",
        "  \n",
        "  \n",
        "class ModelMovielens(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.ranking_model: tf.keras.Model = ModelRanking()\n",
        "    self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
        "      loss = tf.keras.losses.MeanSquaredError(),\n",
        "      metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        "    )\n",
        "\n",
        "  def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
        "    return self.ranking_model(\n",
        "        (features[\"user_id\"], features[\"movie_id\"]))\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    labels = features.pop(\"rating\")\n",
        "    rating_predictions = self(features)\n",
        "\n",
        "    # Compute loss and metric\n",
        "    return self.task(labels=labels, predictions=rating_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lhNbPWpA3RCE"
      },
      "outputs": [],
      "source": [
        "cached_train = trainset.shuffle(100_000).batch(8192).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5C9lyFj0PNKL"
      },
      "outputs": [],
      "source": [
        "model = ModelMovielens()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-Lc1XMzVCCD",
        "outputId": "30108ef8-323d-43df-8c40-56421f54a0d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1362/1362 [==============================] - 190s 138ms/step - root_mean_squared_error: 0.8675 - loss: 0.7524 - regularization_loss: 0.0000e+00 - total_loss: 0.7524 - root_mean_squared_error: 0.8688 -\n",
            "Epoch 2/20\n",
            "1362/1362 [==============================] - 66s 49ms/step - root_mean_squared_error: 0.7893 - loss: 0.6230 - regularization_loss: 0.0000e+00 - total_loss: 0.6230\n",
            "Epoch 3/20\n",
            "1362/1362 [==============================] - 67s 49ms/step - root_mean_squared_error: 0.7712 - loss: 0.5948 - regularization_loss: 0.0000e+00 - total_loss: 0.5948\n",
            "Epoch 4/20\n",
            "1362/1362 [==============================] - 71s 52ms/step - root_mean_squared_error: 0.7600 - loss: 0.5776 - regularization_loss: 0.0000e+00 - total_loss: 0.57762s - root_mean_squared_error: 0.7601 -\n",
            "Epoch 5/20\n",
            "1362/1362 [==============================] - 68s 50ms/step - root_mean_squared_error: 0.7483 - loss: 0.5600 - regularization_loss: 0.0000e+00 - total_loss: 0.5600\n",
            "Epoch 6/20\n",
            "1362/1362 [==============================] - 70s 52ms/step - root_mean_squared_error: 0.7387 - loss: 0.5457 - regularization_loss: 0.0000e+00 - total_loss: 0.5457\n",
            "Epoch 7/20\n",
            "1362/1362 [==============================] - 73s 54ms/step - root_mean_squared_error: 0.7313 - loss: 0.5349 - regularization_loss: 0.0000e+00 - total_loss: 0.5349\n",
            "Epoch 8/20\n",
            "1362/1362 [==============================] - 74s 54ms/step - root_mean_squared_error: 0.7249 - loss: 0.5254 - regularization_loss: 0.0000e+00 - total_loss: 0.5254\n",
            "Epoch 9/20\n",
            "1362/1362 [==============================] - 69s 51ms/step - root_mean_squared_error: 0.7191 - loss: 0.5172 - regularization_loss: 0.0000e+00 - total_loss: 0.5172\n",
            "Epoch 10/20\n",
            "1362/1362 [==============================] - 83s 61ms/step - root_mean_squared_error: 0.7142 - loss: 0.5101 - regularization_loss: 0.0000e+00 - total_loss: 0.5101: 12s - root_mean_squar - ETA: 1s - root_mean_squared_error: 0.7143 - loss: 0.5102 - \n",
            "Epoch 11/20\n",
            "1362/1362 [==============================] - 80s 59ms/step - root_mean_squared_error: 0.7097 - loss: 0.5036 - regularization_loss: 0.0000e+00 - total_loss: 0.5036\n",
            "Epoch 12/20\n",
            "1362/1362 [==============================] - 79s 58ms/step - root_mean_squared_error: 0.7057 - loss: 0.4980 - regularization_loss: 0.0000e+00 - total_loss: 0.4980\n",
            "Epoch 13/20\n",
            "1362/1362 [==============================] - 80s 59ms/step - root_mean_squared_error: 0.7023 - loss: 0.4932 - regularization_loss: 0.0000e+00 - total_loss: 0.4932\n",
            "Epoch 14/20\n",
            "1362/1362 [==============================] - 71s 52ms/step - root_mean_squared_error: 0.6993 - loss: 0.4890 - regularization_loss: 0.0000e+00 - total_loss: 0.48901s - root_mean_squared_error: 0.6992 - loss: 0.4889 - regula\n",
            "Epoch 15/20\n",
            "1362/1362 [==============================] - 69s 51ms/step - root_mean_squared_error: 0.6966 - loss: 0.4852 - regularization_loss: 0.0000e+00 - total_loss: 0.4852\n",
            "Epoch 16/20\n",
            "1362/1362 [==============================] - 65s 48ms/step - root_mean_squared_error: 0.6941 - loss: 0.4818 - regularization_loss: 0.0000e+00 - total_loss: 0.4818\n",
            "Epoch 17/20\n",
            "1362/1362 [==============================] - 68s 50ms/step - root_mean_squared_error: 0.6919 - loss: 0.4787 - regularization_loss: 0.0000e+00 - total_loss: 0.4787\n",
            "Epoch 18/20\n",
            "1362/1362 [==============================] - 68s 50ms/step - root_mean_squared_error: 0.6899 - loss: 0.4760 - regularization_loss: 0.0000e+00 - total_loss: 0.4760\n",
            "Epoch 19/20\n",
            "1362/1362 [==============================] - 71s 52ms/step - root_mean_squared_error: 0.6881 - loss: 0.4735 - regularization_loss: 0.0000e+00 - total_loss: 0.4735\n",
            "Epoch 20/20\n",
            "1362/1362 [==============================] - 69s 50ms/step - root_mean_squared_error: 0.6864 - loss: 0.4712 - regularization_loss: 0.0000e+00 - total_loss: 0.4712\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(cached_train, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_weights('data/recommendation_model_weights.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qnlnbXTaVCO0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recommendations:\n",
            "Toy Story (1995): [[3.506403]]\n",
            "Jefferson in Paris (1995): [[2.778992]]\n",
            "Dracula: Dead and Loving It (1995): [[1.8170128]]\n"
          ]
        }
      ],
      "source": [
        "test_ratings = {}\n",
        "test_movie_ids = [\"0\", \"11\", \"199\"]\n",
        "\n",
        "user_id_test = \"42\"\n",
        "\n",
        "\n",
        "for movie_id in test_movie_ids:\n",
        "  movie_name = movie_dict.get(int(movie_id))\n",
        "  test_ratings[movie_name] = model({\n",
        "      \"user_id\": np.array([user_id_test]),\n",
        "      \"movie_id\": np.array([movie_id])\n",
        "  })\n",
        "\n",
        "print(\"Recommendations:\")\n",
        "for title, score in sorted(test_ratings.items(), key=lambda x: x[1], reverse=True):\n",
        "  print(f\"{title}: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
