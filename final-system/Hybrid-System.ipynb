{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict, Text\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/master_data.zip', compression=\"zip\")[[\"userId\", \"movieId\", \"rating\"]]\n",
    "#Movie names\n",
    "movie_dict = joblib.load(\"data/movie_dict.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1 - Most Similar User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built user-item matrix\n",
    "\n",
    "pivot = df.pivot(\n",
    "    index='userId',\n",
    "    columns='movieId',\n",
    "    values='rating'\n",
    ").fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "#Movie ID - Rating\n",
    "new_user_ratings = {4181: 4.5, 4188: 4.5, 4195:4.5, 4198:3.0, 4204:5.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\metehan.gundogdu\\AppData\\Local\\Temp\\ipykernel_16720\\3256984122.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  pivot = pivot.append(new_user_ratings, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "pivot = pivot.append(new_user_ratings, ignore_index=True)\n",
    "\n",
    "pivot.fillna(0, inplace=True)\n",
    "df_sparse = csr_matrix(pivot.values)\n",
    "\n",
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "model_knn.fit(df_sparse)\n",
    "\n",
    "distances, indices = model_knn.kneighbors(pivot.tail(1), n_neighbors=2)\n",
    "\n",
    "most_similar_user = indices[0][1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2 - Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model has to be re-built due to load issues\n",
    "\n",
    "unique_movie_ids = joblib.load(\"data/unique_movie_ids.pkl\")\n",
    "unique_user_ids = joblib.load(\"data/unique_user_ids.pkl\")\n",
    "\n",
    "class ModelRanking(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    embedding_dims = 32\n",
    "\n",
    "    # User embeddings\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_user_ids, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dims)\n",
    "    ])\n",
    "\n",
    "    # Movie Embeddings\n",
    "    self.movie_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_movie_ids, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_ids) + 1, embedding_dims)\n",
    "    ])\n",
    "\n",
    "    # Predictions\n",
    "    self.ratings = tf.keras.Sequential([\n",
    "      # multiple dense layers\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      # Ratings in output layer\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    user_id, movie_id = inputs\n",
    "\n",
    "    user_embed = self.user_embeddings(user_id)\n",
    "    movie_embed = self.movie_embeddings(movie_id)\n",
    "\n",
    "    return self.ratings(tf.concat([user_embed, movie_embed], axis=1))\n",
    "  \n",
    "  \n",
    "class ModelMovielens(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ranking_model: tf.keras.Model = ModelRanking()\n",
    "    self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "      loss = tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "\n",
    "  def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "    return self.ranking_model(\n",
    "        (features[\"user_id\"], features[\"movie_id\"]))\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    labels = features.pop(\"rating\")\n",
    "    rating_predictions = self(features)\n",
    "\n",
    "    # Compute loss and metric\n",
    "    return self.task(labels=labels, predictions=rating_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_model = ModelMovielens()\n",
    "# Dummy input to reconstruct the model\n",
    "recommendation_model({\n",
    "      \"user_id\": np.array([\"0\"]),\n",
    "      \"movie_id\": np.array([\"0\"])\n",
    "  })\n",
    "\n",
    "recommendation_model.load_weights('data/recommendation_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies to predict\n",
    "all_movies = np.array(list(movie_dict.keys()))\n",
    "\n",
    "excluded_movies = np.array(joblib.load(\"data/excluded_movie_ids.pkl\"))\n",
    "exc_mask = np.isin(all_movies, excluded_movies, invert=True)\n",
    "candidate_movies = all_movies[exc_mask]\n",
    "\n",
    "watched_movies = np.array(list(new_user_ratings.keys()))\n",
    "mask = np.isin(candidate_movies, watched_movies, invert=True)\n",
    "candidate_movies = candidate_movies[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_ratings = {}\n",
    "\n",
    "movies_to_predict = list(candidate_movies.astype(str))\n",
    "user_to_predict = str(most_similar_user)\n",
    "\n",
    "for movie_id in movies_to_predict:\n",
    "  #movie_name = movie_dict.get(int(movie_id))\n",
    "  \n",
    "  predicted_rating = recommendation_model({\n",
    "      \"user_id\": np.array([user_to_predict]),\n",
    "      \"movie_id\": np.array([movie_id])\n",
    "  }).numpy()[0][0]\n",
    "  \n",
    "  recommendation_ratings[movie_id] = predicted_rating    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the values to be used in Layer 4\n",
    "\n",
    "rec_values = np.array(list(recommendation_ratings.values())).reshape(-1, 1)\n",
    "\n",
    "recommend_scaler = MinMaxScaler()\n",
    "rec_scaled = recommend_scaler.fit_transform(rec_values)\n",
    "\n",
    "recommendation_ratings = {key: value for key, value in zip(recommendation_ratings.keys(), rec_scaled.flatten())}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3 - Demand Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\metehan.gundogdu\\miniconda3\\envs\\timework\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from darts.models import TFTModel\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_model = TFTModel.load(\"data/forecasting/forecasting-model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ts_scaled_dict = joblib.load(\"data/forecasting/target_ts_scaled_dict.pkl\")\n",
    "\n",
    "id_to_order = dict(zip(list(target_ts_scaled_dict.keys()), range(len(target_ts_scaled_dict.keys()))))\n",
    "id_to_order = {str(key): value for key,value in id_to_order.items()}\n",
    "order_to_id = {value:key for key,value in id_to_order.items()}\n",
    "\n",
    "target_ts_scaled_dict = {str(key): value for key, value in target_ts_scaled_dict.items() if str(key) in recommendation_ratings}\n",
    "\n",
    "covariate_ts_scaled_dict = joblib.load(\"data/forecasting/covariate_ts_scaled_dict.pkl\")\n",
    "covariate_ts_scaled_dict = {str(key): value for key, value in covariate_ts_scaled_dict.items() if str(key) in recommendation_ratings}\n",
    "\n",
    "target_scaler = joblib.load(\"data/forecasting/target_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [x[:-12] for x in target_ts_scaled_dict.values()]\n",
    "covariates = list(covariate_ts_scaled_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\metehan.gundogdu\\miniconda3\\envs\\timework\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 139/139 [00:35<00:00,  3.92it/s]\n"
     ]
    }
   ],
   "source": [
    "forecast_preds = forecast_model.predict(n=12, series=targets, past_covariates=covariates)\n",
    "forecast_preds = dict(zip(list(target_ts_scaled_dict.keys()), forecast_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ts = [TimeSeries.from_values(np.array([None])) for _ in range(len(id_to_order))]\n",
    "\n",
    "for i in target_ts_scaled_dict.keys():\n",
    "    order = id_to_order[i]\n",
    "    temp_ts[order] = forecast_preds[i]\n",
    "\n",
    "inversed = target_scaler.inverse_transform(temp_ts, n_jobs=-1)\n",
    "\n",
    "forecast_preds = {}\n",
    "for n, i in enumerate(inversed):\n",
    "    id_n = order_to_id[n]\n",
    "    if id_n in target_ts_scaled_dict.keys():\n",
    "        forecast_preds[id_n] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scaler"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_arrays = [x.values() for x in forecast_preds.values()]\n",
    "pred_arrays = np.concatenate(pred_arrays, axis=0).reshape(-1,)\n",
    "pred_arrays =TimeSeries.from_values(pred_arrays)\n",
    "\n",
    "aggregate_scaler = Scaler()\n",
    "aggregate_scaler.fit(pred_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USER INPUT\n",
    "\n",
    "chosen_month = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the values of the chosen month to be used in Layer 4\n",
    "\n",
    "monthly_preds = {}\n",
    "for key, value in forecast_preds.items():\n",
    "    month_indexer = value.time_index.month.get_loc(chosen_month)\n",
    "    monthly_preds[key] = value[month_indexer].values()[0][0]\n",
    "    \n",
    "scaled_preds = aggregate_scaler.transform(TimeSeries.from_values(np.array(list(monthly_preds.values()))))\n",
    "scaled_preds = scaled_preds.values().reshape(-1,).tolist()\n",
    "\n",
    "monthly_preds = {key: value for key, value in zip(monthly_preds.keys(), scaled_preds)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4 - Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_algo (normalized_rating, normalized_forecast):\n",
    "        highest = np.max([normalized_rating, normalized_forecast])\n",
    "        output = highest + np.mean([normalized_rating, normalized_forecast])\n",
    "        \n",
    "        #Maximum possible value for output is 2 and minimum is 0. Values are normalized according to this.\n",
    "        normalized_ensembled_output = output/2\n",
    "        return normalized_ensembled_output\n",
    "\n",
    "aggregated = {key: ensemble_algo(recommendation_ratings[key], monthly_preds[key]) for key in recommendation_ratings}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Seven (a.k.a. Se7en) (1995)',\n",
       " 'Matrix, The (1999)',\n",
       " 'WALLÂ·E (2008)',\n",
       " 'Slumdog Millionaire (2008)',\n",
       " 'Prometheus (2012)',\n",
       " 'Star Trek (2009)',\n",
       " 'Sin City (2005)',\n",
       " 'X-Men: First Class (2011)',\n",
       " \"Pan's Labyrinth (Laberinto del fauno, El) (2006)\",\n",
       " 'Death Proof (2007)']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_aggregated = sorted(aggregated, key=aggregated.get, reverse=True)\n",
    "sorted_movie_names = [movie_dict[int(ID)] for ID in sort_aggregated]\n",
    "\n",
    "sorted_movie_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
