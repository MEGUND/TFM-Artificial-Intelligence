{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IUxJ0pt8D93"
      },
      "source": [
        "# TFRS\n",
        "\n",
        "This notebook takes the master data that was prepared in the \"data_preprocessing\" notebook, creates a test holdout set from the 20% of the data that the same split are used for other experimentations for consistency.\n",
        "\n",
        "The model is created by TensorFlow Recommenders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2xnqHu9X_JuV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "from typing import Dict, Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi_nO_5d8ryx"
      },
      "source": [
        "## Train-test Split\n",
        "\n",
        "The train test split is done by only taking the 20% of the data as the test holdout set. For making sure the train and test data is consistent in all experiments, the following test holdout split will be the same for each experiment.\n",
        "\n",
        "It is an important detail that the split is done in a stratified way to ensure that the user rankings will be splitted as evenly as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iOE86adV_TfG"
      },
      "outputs": [],
      "source": [
        "#Do not load the \"timestamp\" column since it is not needed for building the recommender engine\n",
        "df = pd.read_csv('data/master_data.zip', compression=\"zip\")[[\"userId\", \"movieId\", \"rating\"]]\n",
        "#This split will be standard for all experiments\n",
        "from sklearn import model_selection\n",
        "\n",
        "X = df.copy()\n",
        "y = df[\"userId\"]\n",
        "\n",
        "#There is no need for the target values since we are splitting the whole dataset\n",
        "#y is only given for stratifying\n",
        "\n",
        "trainset, testset, _, _ = model_selection.train_test_split(X, y, test_size = 0.20, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-27CJ9N8u-B"
      },
      "source": [
        "Data is transformed into TensorFlow dataset format "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aOy9ffu_jM1",
        "outputId": "1aca16dc-e260-4e34-c7cd-a978046501ec"
      },
      "outputs": [],
      "source": [
        "trainset = tf.data.Dataset.from_tensor_slices(trainset.values)\n",
        "\n",
        "trainset = trainset.map(lambda x: {\n",
        "    \"user_id\": tf.as_string(tf.cast(x[0], tf.int32))  ,\n",
        "    \"movie_id\": tf.as_string(tf.cast(x[1], tf.int32)) ,\n",
        "    \"rating\": tf.cast(x[2], tf.float32)\n",
        "})\n",
        "\n",
        "testset = tf.data.Dataset.from_tensor_slices(testset.values)\n",
        "\n",
        "testset = testset.map(lambda x: {\n",
        "    \"user_id\": tf.as_string(tf.cast(x[0], tf.int32))  ,\n",
        "    \"movie_id\": tf.as_string(tf.cast(x[1], tf.int32)) ,\n",
        "    \"rating\": tf.cast(x[2], tf.float32)\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNTbi3vr85FA"
      },
      "source": [
        "Unique user and movie IDs are determined for embedding generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OYHm12yS_jPX"
      },
      "outputs": [],
      "source": [
        "movie_ids = trainset.batch(1_000_000).map(lambda x: x[\"movie_id\"])\n",
        "user_ids = trainset.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
        "\n",
        "unique_movie_ids = np.unique(np.concatenate(list(movie_ids)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H31rIU1o-Lbw"
      },
      "source": [
        "## Model Build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rEFgWIEz_jR9"
      },
      "outputs": [],
      "source": [
        "class ModelRanking(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    embedding_dims = 32\n",
        "\n",
        "    # User embeddings\n",
        "    self.user_embeddings = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_user_ids, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dims)\n",
        "    ])\n",
        "\n",
        "    # Movie Embeddings\n",
        "    self.movie_embeddings = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_movie_ids, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_ids) + 1, embedding_dims)\n",
        "    ])\n",
        "\n",
        "    # Predictions\n",
        "    self.ratings = tf.keras.Sequential([\n",
        "      # multiple dense layers\n",
        "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "      # Ratings in output layer\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    user_id, movie_id = inputs\n",
        "\n",
        "    user_embed = self.user_embeddings(user_id)\n",
        "    movie_embed = self.movie_embeddings(movie_id)\n",
        "\n",
        "    return self.ratings(tf.concat([user_embed, movie_embed], axis=1))\n",
        "  \n",
        "  \n",
        "  \n",
        "class ModelMovielens(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.ranking_model: tf.keras.Model = ModelRanking()\n",
        "    self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
        "      loss = tf.keras.losses.MeanSquaredError(),\n",
        "      metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        "    )\n",
        "\n",
        "  def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
        "    return self.ranking_model(\n",
        "        (features[\"user_id\"], features[\"movie_id\"]))\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    labels = features.pop(\"rating\")\n",
        "    rating_predictions = self(features)\n",
        "\n",
        "    # Compute loss and metric\n",
        "    return self.task(labels=labels, predictions=rating_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6HgXAApDABjV"
      },
      "outputs": [],
      "source": [
        "model = ModelMovielens()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lhNbPWpA3RCE"
      },
      "outputs": [],
      "source": [
        "cached_train = trainset.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = testset.batch(4096).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBpBpwZgABwi",
        "outputId": "d5aeb285-ff6e-46e0-d56f-58953650470d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1090/1090 [==============================] - 297s 270ms/step - root_mean_squared_error: 0.8674 - loss: 0.7523 - regularization_loss: 0.0000e+00 - total_loss: 0.7523\n",
            "Epoch 2/20\n",
            "1090/1090 [==============================] - 6s 5ms/step - root_mean_squared_error: 0.7951 - loss: 0.6322 - regularization_loss: 0.0000e+00 - total_loss: 0.6322\n",
            "Epoch 3/20\n",
            "1090/1090 [==============================] - 6s 5ms/step - root_mean_squared_error: 0.7771 - loss: 0.6038 - regularization_loss: 0.0000e+00 - total_loss: 0.6038\n",
            "Epoch 4/20\n",
            "1090/1090 [==============================] - 5s 5ms/step - root_mean_squared_error: 0.7681 - loss: 0.5899 - regularization_loss: 0.0000e+00 - total_loss: 0.5899\n",
            "Epoch 5/20\n",
            "1090/1090 [==============================] - 6s 5ms/step - root_mean_squared_error: 0.7622 - loss: 0.5808 - regularization_loss: 0.0000e+00 - total_loss: 0.5808\n",
            "Epoch 6/20\n",
            "1090/1090 [==============================] - 5s 5ms/step - root_mean_squared_error: 0.7566 - loss: 0.5724 - regularization_loss: 0.0000e+00 - total_loss: 0.5724\n",
            "Epoch 7/20\n",
            "1090/1090 [==============================] - 6s 6ms/step - root_mean_squared_error: 0.7510 - loss: 0.5639 - regularization_loss: 0.0000e+00 - total_loss: 0.5639\n",
            "Epoch 8/20\n",
            "1090/1090 [==============================] - 5s 5ms/step - root_mean_squared_error: 0.7450 - loss: 0.5549 - regularization_loss: 0.0000e+00 - total_loss: 0.5549\n",
            "Epoch 9/20\n",
            "1090/1090 [==============================] - 6s 5ms/step - root_mean_squared_error: 0.7395 - loss: 0.5467 - regularization_loss: 0.0000e+00 - total_loss: 0.5467\n",
            "Epoch 10/20\n",
            "1090/1090 [==============================] - 5s 5ms/step - root_mean_squared_error: 0.7349 - loss: 0.5399 - regularization_loss: 0.0000e+00 - total_loss: 0.5399\n",
            "Epoch 11/20\n",
            "1090/1090 [==============================] - 6s 5ms/step - root_mean_squared_error: 0.7303 - loss: 0.5332 - regularization_loss: 0.0000e+00 - total_loss: 0.5332\n",
            "Epoch 12/20\n",
            "1090/1090 [==============================] - 6s 5ms/step - root_mean_squared_error: 0.7259 - loss: 0.5268 - regularization_loss: 0.0000e+00 - total_loss: 0.5268\n",
            "Epoch 13/20\n",
            "1090/1090 [==============================] - 5s 5ms/step - root_mean_squared_error: 0.7221 - loss: 0.5213 - regularization_loss: 0.0000e+00 - total_loss: 0.5213\n",
            "Epoch 14/20\n",
            "1090/1090 [==============================] - 5s 5ms/step - root_mean_squared_error: 0.7190 - loss: 0.5169 - regularization_loss: 0.0000e+00 - total_loss: 0.5169\n",
            "Epoch 15/20\n",
            "1090/1090 [==============================] - 6s 5ms/step - root_mean_squared_error: 0.7162 - loss: 0.5129 - regularization_loss: 0.0000e+00 - total_loss: 0.5129\n",
            "Epoch 16/20\n",
            "1090/1090 [==============================] - 5s 5ms/step - root_mean_squared_error: 0.7132 - loss: 0.5085 - regularization_loss: 0.0000e+00 - total_loss: 0.5085\n",
            "Epoch 17/20\n",
            "1090/1090 [==============================] - 6s 6ms/step - root_mean_squared_error: 0.7110 - loss: 0.5054 - regularization_loss: 0.0000e+00 - total_loss: 0.5054\n",
            "Epoch 18/20\n",
            "1090/1090 [==============================] - 5s 5ms/step - root_mean_squared_error: 0.7097 - loss: 0.5036 - regularization_loss: 0.0000e+00 - total_loss: 0.5036\n",
            "Epoch 19/20\n",
            "1090/1090 [==============================] - 6s 5ms/step - root_mean_squared_error: 0.7079 - loss: 0.5010 - regularization_loss: 0.0000e+00 - total_loss: 0.5010\n",
            "Epoch 20/20\n",
            "1090/1090 [==============================] - 5s 5ms/step - root_mean_squared_error: 0.7055 - loss: 0.4976 - regularization_loss: 0.0000e+00 - total_loss: 0.4976\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3b40a35fd0>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(cached_train, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the weights\n",
        "model.save_weights('./tfrs_weights')\n",
        "\n",
        "# Restore the weights \n",
        "#model.load_weights('tfrs_model/tfrs_weights')\n",
        "\n",
        "#Save the model\n",
        "#model.save('./tfrs_model')\n",
        "#model = tf.keras.models.load_model('tfrs_model/tfrs_model')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfmyVAxs-U3s"
      },
      "source": [
        "## Model Evaluate\n",
        "\n",
        "Here only the RMSE metric is focused to have an overview of the accuracy of the model to be compared with other recommendation systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tx5VR94AVFl",
        "outputId": "db4e3da1-a17f-40d1-9c21-8ac9ca7947bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "545/545 [==============================] - 71s 130ms/step - root_mean_squared_error: 0.7669 - loss: 0.5881 - regularization_loss: 0.0000e+00 - total_loss: 0.5881\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'root_mean_squared_error': 0.766929566860199,\n",
              " 'loss': 0.5473156571388245,\n",
              " 'regularization_loss': 0,\n",
              " 'total_loss': 0.5473156571388245}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(cached_test, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_7DfcyrCR-6"
      },
      "source": [
        "Get Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_names = pd.read_csv('data/master_data_with_movie_info.zip', compression=\"zip\")[[\"movieId\", \"title\"]]\n",
        "df_names = df_names.set_index('movieId')\n",
        "movie_dict = df_names['title'].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkN0inKjAVIw",
        "outputId": "1074a3a6-ccb6-4132-9699-b36970a24a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recommendations:\n",
            "Toy Story (1995): [[3.5536563]]\n",
            "Jefferson in Paris (1995): [[2.3756332]]\n",
            "Dracula: Dead and Loving It (1995): [[2.265089]]\n"
          ]
        }
      ],
      "source": [
        "test_ratings = {}\n",
        "test_movie_ids = [\"0\", \"11\", \"199\"]\n",
        "\n",
        "user_id_test = \"42\"\n",
        "\n",
        "\n",
        "for movie_id in test_movie_ids:\n",
        "  movie_name = movie_dict.get(int(movie_id))\n",
        "  test_ratings[movie_name] = model({\n",
        "      \"user_id\": np.array([user_id_test]),\n",
        "      \"movie_id\": np.array([movie_id])\n",
        "  })\n",
        "\n",
        "print(\"Recommendations:\")\n",
        "for title, score in sorted(test_ratings.items(), key=lambda x: x[1], reverse=True):\n",
        "  print(f\"{title}: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1-grsVt6H9y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
